{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Import packages"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, \\\n",
    "    r2_score, make_scorer, recall_score, accuracy_score, f1_score, \\\n",
    "    precision_score, balanced_accuracy_score, roc_curve, auc"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Custom functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pickle_managment import save_pickle, load_pickle"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Regression"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "regression_df_expanded_cleaned = pd.read_csv(\n",
    "    r'datasets\\train_datasets\\regression_df_expanded_cleaned_train.csv.zip'\n",
    ")\n",
    "regression_df_expanded_cleaned"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "regression_X = regression_df_expanded_cleaned.loc[\n",
    "               :,\n",
    "               ~regression_df_expanded_cleaned.columns.isin(\n",
    "                   ['SMILES', 'logBB'])\n",
    "               ]\n",
    "\n",
    "regression_y = regression_df_expanded_cleaned['logBB']\n",
    "\n",
    "# Use the transformer that the SVM regressor has used before\n",
    "data_processing_pipeline =load_pickle(r'model_outputs\\svm_regressor\\svm_regressor_pipeline.pkl')\n",
    "regression_X_processed=data_processing_pipeline.transform(regression_X)\n",
    "regression_X_processed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "regression_X_train, regression_X_test, regression_y_train, regression_y_test = train_test_split(\n",
    "    regression_X_processed,\n",
    "    regression_y,\n",
    "    test_size=0.2,\n",
    "    random_state=1,\n",
    "    shuffle=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_pickle(regression_X_train,\n",
    "            r'model_outputs\\rf_regressor\\regression_X_train.pkl')\n",
    "\n",
    "save_pickle(regression_y_train,\n",
    "            r'model_outputs\\rf_regressor\\regression_y_train.pkl')\n",
    "\n",
    "save_pickle(regression_X_test,\n",
    "            r'model_outputs\\rf_regressor\\regression_X_test.pkl')\n",
    "\n",
    "save_pickle(regression_y_test,\n",
    "            r'model_outputs\\rf_regressor\\regression_y_test.pkl')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Random forest"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = datetime.now()\n",
    "\n",
    "rf_regressor = RandomForestRegressor(\n",
    "    random_state=1,\n",
    "    bootstrap=True, #Whether bootstrap samples are used when building trees.\n",
    "    # Default to be True\n",
    "    oob_score=True #Whether to use out-of-bag samples to estimate the\n",
    "    # generalization score. Only available if bootstrap=True\n",
    ")\n",
    "\n",
    "rf_regressor_grid_search = GridSearchCV(\n",
    "    estimator=rf_regressor,\n",
    "    param_grid={\n",
    "        'n_estimators': [1000, 3500],  #Number of trees\n",
    "        'max_depth': [5, 20],  #The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples\n",
    "        'min_samples_split': [2, 30], #The minimum number of samples required\n",
    "        # to split an internal node. Default to be 2\n",
    "        'min_samples_leaf': [5, 30], #The minimum number of samples required\n",
    "        # to be at a leaf node. The branch will stop splitting once the\n",
    "        # leaves have that number of samples each. Having 1 here is more\n",
    "        # prone to overfitting\n",
    "        # 'criterion': ['squared_error'] # “squared_error” for the mean\n",
    "        # squared error. This is the default\n",
    "        'max_samples':[0.2, 0.85],\n",
    "        'max_features': ['sqrt', 'log2'] #Reduce the number of features can\n",
    "        # mitigate overfitting issue\n",
    "    },\n",
    "    cv=8,  #Number of fold for cross validation. It should be 8 or 10\n",
    "    scoring={\n",
    "        # All these are only viable in the negative option\n",
    "        'MAE': 'neg_mean_absolute_error',\n",
    "        'MSE': 'neg_mean_squared_error',\n",
    "        'R2': 'r2'\n",
    "    },\n",
    "    refit='R2', #MAE is less sensitive to outliers and can help reduce\n",
    "    # overfitting\n",
    "\n",
    "    n_jobs=1,\n",
    "    # -1 means using all processors, but it won't give you any messages.\n",
    "    # Only using 1 for my computer print out the training messages\n",
    "\n",
    "    verbose=10  #Provide detailed more messages\n",
    ")\n",
    "\n",
    "rf_regressor_grid_search.fit(regression_X_train, regression_y_train)\n",
    "\n",
    "end_time = datetime.now()\n",
    "print('GridSearchCV took {}'.format(end_time - start_time))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rf_regressor_results_df = pd.DataFrame(rf_regressor_grid_search.cv_results_)\n",
    "#Make the GridSearch results into a df\n",
    "rf_regressor_results_df.drop(\n",
    "    list(rf_regressor_results_df.filter(regex='time|split|std')),\n",
    "    axis=1,\n",
    "    inplace=True\n",
    ")  # Remove columns that aren't very interesting\n",
    "\n",
    "rf_regressor_results_df = rf_regressor_results_df.sort_values(\n",
    "    by='rank_test_R2')\n",
    "rf_regressor_results_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rf_regressor_results_df.to_csv(\n",
    "    r'model_grid_search\\rf_regressor_results.csv',\n",
    "    index=False\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_rf_regressor = rf_regressor_grid_search.best_estimator_\n",
    "save_pickle(\n",
    "    best_rf_regressor,\n",
    "    r'model_pickles\\best_rf_regressor.pkl'\n",
    ")\n",
    "# To load this best model again, use load_pickle(r'model_pickles\\random_forest_regressor\\best_rf_regressor.pkl')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Classification"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Neural network\n",
    "According to SVM classifiers' performance using the 2 methods of balancing\n",
    "the dataset, the one created by centroid gave more reliable better results.\n",
    "Consequently, that dataset is used to train the MLP classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "classification_df_expanded_balanced = pd.read_csv(\n",
    "    r'datasets\\balanced_datasets\\BBB_classification_balanced_centroid.csv.zip'\n",
    ")\n",
    "classification_df_expanded_balanced"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Data pre-processing. This will make the model less interpretable but since\n",
    "# MLP is already less interpretable than RF, this is fine since I will try\n",
    "# to use another RF to explain why MLP made such decision\n",
    "\n",
    "classification_X = classification_df_expanded_balanced.loc[\n",
    "                   :,\n",
    "                   ~classification_df_expanded_balanced.columns.isin(\n",
    "                       ['SMILES', 'BBB+/BBB-'])\n",
    "                   ]\n",
    "\n",
    "classification_y = classification_df_expanded_balanced['BBB+/BBB-']\n",
    "\n",
    "\n",
    "data_processing_pipeline =load_pickle\\\n",
    "    (r'model_outputs\\svm_classifier\\centroid_pipeline.pkl')\n",
    "\n",
    "classification_X_processed=data_processing_pipeline.transform(classification_X)\n",
    "classification_X_processed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "classification_X_train, classification_X_test, classification_y_train, classification_y_test = train_test_split(\n",
    "    classification_X_processed,\n",
    "    classification_y,\n",
    "    test_size=0.2,\n",
    "    random_state=1,\n",
    "    shuffle=True,\n",
    "    stratify=classification_y #Ensure train set and test set have the same\n",
    "    # ratio for the 2 categories\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_pickle(classification_X_train,\n",
    "            r'model_outputs\\mlp_classifier\\classification_X_train.pkl')\n",
    "\n",
    "save_pickle(classification_y_train,\n",
    "            r'model_outputs\\mlp_classifier\\classification_y_train.pkl')\n",
    "\n",
    "save_pickle(classification_X_test,\n",
    "            r'model_outputs\\mlp_classifier\\classification_X_test.pkl')\n",
    "\n",
    "save_pickle(classification_y_test,\n",
    "            r'model_outputs\\mlp_classifier\\classification_y_test.pkl')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = datetime.now()\n",
    "\n",
    "mlp_classifier = MLPClassifier(\n",
    "    solver='adam',\n",
    "    #\"adam\" is the default. It works pretty well on relatively large datasets (with thousands of training samples or more) in terms of both training time and validation score\n",
    "    random_state=1,\n",
    "    shuffle=True,  #shuffle samples in each iteration for \"adam\" solver\n",
    ")\n",
    "\n",
    "mlp_classifier_grid_search = GridSearchCV(\n",
    "    estimator=mlp_classifier,\n",
    "    param_grid={\n",
    "        'hidden_layer_sizes': [(10,),(50,),(10,10)],\n",
    "        'alpha': [1e-3, 1], #1e-4 is the default. Need stronger\n",
    "        # regularization to avoid overfitting\n",
    "        'activation': [\n",
    "            'relu', #\"relu\" is the default\n",
    "            'tanh'\n",
    "        ],\n",
    "        'batch_size': [32, 128], #Smaller batch size can help reducing\n",
    "        # overfitting. Since the dataset has feature number > 200, the\n",
    "        # default is to use 200 as the batch_size\n",
    "        'learning_rate': ['constant', 'adaptive'] #Default to be \"constant\"\n",
    "    },\n",
    "    cv=RepeatedStratifiedKFold(\n",
    "        n_splits=8,\n",
    "        n_repeats=2, #Each time the split will be different\n",
    "        random_state=1\n",
    "    ),\n",
    "    scoring={\n",
    "        'Recall': make_scorer(\n",
    "            recall_score, #Need pos_label\n",
    "            pos_label='BBB+', #Without this, pos_label is default to be 1\n",
    "            # and will through an error since 1 isn't \"BBB+\" or \"BBB-\"\n",
    "            average='binary'\n",
    "        ),\n",
    "        'Precision': make_scorer(\n",
    "            precision_score, #Need pos_label\n",
    "            pos_label='BBB+',\n",
    "            average='binary'\n",
    "        ),\n",
    "        'F1': make_scorer(\n",
    "            f1_score, #Need pos_label\n",
    "            pos_label='BBB+',\n",
    "            average='binary'\n",
    "        ),\n",
    "        'Accuracy': 'accuracy', #accuracy_score doesn't need pos_label\n",
    "        'Balanced accuracy': 'balanced_accuracy',\n",
    "        'AUROC': 'roc_auc'\n",
    "    },\n",
    "    refit='AUROC',\n",
    "\n",
    "    n_jobs=1,\n",
    "    verbose=10\n",
    ")\n",
    "\n",
    "mlp_classifier_grid_search.fit(classification_X_train, classification_y_train)\n",
    "\n",
    "end_time = datetime.now()\n",
    "print('GridSearchCV took {}'.format(end_time - start_time))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mlp_classifier_results_df = pd.DataFrame(mlp_classifier_grid_search.cv_results_)\n",
    "#Make the GridSearch results into a df\n",
    "\n",
    "mlp_classifier_results_df.drop(\n",
    "    list(mlp_classifier_results_df.filter(regex='time|split|std')),\n",
    "    axis=1,\n",
    "    inplace=True\n",
    ")  # Remove columns that aren't very interesting\n",
    "mlp_classifier_results_df = mlp_classifier_results_df.sort_values(\n",
    "    by='rank_test_AUROC')\n",
    "\n",
    "mlp_classifier_results_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mlp_classifier_results_df.to_csv(\n",
    "    r'model_grid_search\\mlp_classifier_results.csv',\n",
    "    index=False\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_mlp_classifier = mlp_classifier_grid_search.best_estimator_\n",
    "save_pickle(\n",
    "    best_mlp_classifier,\n",
    "    r'model_pickles\\best_mlp_classifier.pkl'\n",
    ")\n",
    "# To load this best model again, use load_pickle\n",
    "# (r'model_pickles\\mlp_classifier\\best_mlp_classifier.pkl')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
